{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neurohack - Challenge 2 London team e\n",
    "### Author: Maitreyee Wairagkar\n",
    "#### Last update: 13/01/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "'''\n",
    "# Convert .dat to csv\n",
    "data = pd.io.stata.read_stata('H_DAD_w1a3.dta')\n",
    "data.to_csv('H_DAD_w1a3.csv')\n",
    "'''\n",
    "\n",
    "data = pd.read_csv('H_DAD_w1a3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing by removing non-relevant features and NaN columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_features = ['prim_key','hhid','pnc','pn','r1stateid','r1iwy_d','r1iwm_d','r1lasidy','rabyear', \n",
    "                   'rabmonth', 'r1lang_d', 'r1location', 'r1mheight', 'r1mweight', 'r1ht_flag', \n",
    "                   'r1wt_flag', 'r1htlimbs', 'r1midarm', 'r1calf', 'r1kneeht', 'r1raterid1', \n",
    "                   'r1raterid2', 'r1raterid3', 'r1phase', 'r1iwstat_d', 'r1inf_age', 'r1inf_gendr', \n",
    "                   'r1inf_educ', 'r1inf_rel', 'r1inf_freq', 'r1inf_care', 'r1inf_yrs', 'r1mbmi', 'r1bmicat',] \n",
    "\n",
    "potential_labels = ['r1cdr_final','r1cdr_incon','r1cdr_mem1','r1cdr_ori1','r1cdr_jud1','r1cdr_com1',\n",
    "                    'r1cdr_hom1','r1cdr_per1','r1cdr_scor1','r1cdr_mem2','r1cdr_ori2','r1cdr_jud2',\n",
    "                    'r1cdr_com2','r1cdr_hom2','r1cdr_per2','r1cdr_scor2','r1cdr_mem3','r1cdr_ori3',\n",
    "                    'r1cdr_jud3','r1cdr_com3','r1cdr_hom3','r1cdr_per3','r1cdr_scor3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing: remove non-important features\n",
    "\n",
    "# remove rows with missing values for clinician evaluation (this discards phase 1)\n",
    "df = data.dropna(subset = ['r1cdr_final'])\n",
    "\n",
    "labels = df['r1cdr_final']\n",
    "gender = df['ragender']\n",
    "\n",
    "df = df.drop(columns=potential_labels)\n",
    "df = df.drop(columns=remove_features)\n",
    "\n",
    "# remove columns with NaN\n",
    "df = df.dropna(axis=1)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-SNE Visualisation of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot T-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# We want to get TSNE embedding with 2 dimensions\n",
    "n_components = 3\n",
    "tsne = TSNE(n_components)\n",
    "tsne_result = tsne.fit_transform(df)\n",
    "tsne_result.shape\n",
    "\n",
    "# Plot T-SNE\n",
    "scatter_x = np.array(tsne_result[:,0])\n",
    "scatter_y = np.array(tsne_result[:,1])\n",
    "scatter_z = np.array(tsne_result[:,2])\n",
    "group = np.array(labels)\n",
    "cdict = {0: 'green', 0.5: 'blue', 1:'orange', 2:'red', 3: 'purple'}\n",
    "\n",
    "fig = plt.figure(figsize = (7, 7))\n",
    "ax = plt.axes(projection =\"3d\")\n",
    "\n",
    "for g in np.unique(group):\n",
    "    ix = np.where(group == g)\n",
    "    ax.scatter3D(scatter_x[ix], scatter_y[ix], scatter_z[ix], c = cdict[g], label = g)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram of pre-processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "# Creating dataset\n",
    "a = []\n",
    "for i in labels:\n",
    "    if i == 0: # no dementia\n",
    "        a.append(i)\n",
    "    if i == 0.5: # questionable\n",
    "        a.append(1)\n",
    "    if i == 1: # mild\n",
    "        a.append(2)\n",
    "    if i == 2: # moderate\n",
    "        a.append(3)\n",
    "    if i == 3: # severe\n",
    "        a.append(4)\n",
    "\n",
    "# Creating histogram\n",
    "labl, counts = np.unique(a, return_counts=True)\n",
    "\n",
    "fig = plt.figure(figsize = (9, 6))\n",
    "plt.rc('font', size=15) \n",
    "\n",
    "plt.bar(labl, counts, align='center')\n",
    "plt.gca().set_xticks(labl)\n",
    "plt.gca().set_xticklabels(('No\\nimpairment', 'Questionable\\nimpairment', 'Mild\\ndementia', 'Moderate\\ndementia', 'Severe\\ndementia'))\n",
    "plt.xlabel('\\nConsensus Clinical Dementia Rating Score')\n",
    "plt.ylabel('Number of people')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA on cleaned data (all features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardise the data before PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "x = MinMaxScaler().fit_transform(df) # normalizing the features\n",
    "x.shape\n",
    "\n",
    "\n",
    "# PCA on all features\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA() #n_components=3\n",
    "principalComponents = pca.fit_transform(x)\n",
    "\n",
    "plt.plot(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot components\n",
    "\n",
    "#%matplotlib qt\n",
    "\n",
    "comp =[0,2,4] # select components to plot\n",
    "scatter_x = np.array(principalComponents[:,comp[0]])\n",
    "scatter_y = np.array(principalComponents[:,comp[1]])\n",
    "scatter_z = np.array(principalComponents[:,comp[2]])\n",
    "group = np.array(labels)\n",
    "cdict = {0: 'green', 0.5: 'blue', 1:'orange', 2:'red', 3: 'purple'}\n",
    "label_names = {0:'No impairment', 0.5:'Questionable impairment', 1:'Mild dementia', 2:'Moderate dementia', 3:'Severe dementia'}\n",
    "\n",
    "fig = plt.figure(figsize = (7, 7))\n",
    "plt.rc('font', size=10) \n",
    "ax = plt.axes(projection =\"3d\")\n",
    "\n",
    "\n",
    "for g in np.unique(group):\n",
    "    ix = np.where(group == g)\n",
    "    ax.scatter(scatter_x[ix], scatter_y[ix], scatter_z[ix], c = cdict[g], label = label_names[g] )\n",
    "    \n",
    "ax.legend(bbox_to_anchor=(0, 1.15),loc='upper left', title='Clinical Dementia Rating')\n",
    "plt.xlabel('Principal Component '+str(comp[0]+1))\n",
    "plt.ylabel('Principal Component '+str(comp[1]+1))\n",
    "ax.set_zlabel('Principal Component '+str(comp[2]+1))\n",
    "\n",
    "#ax.view_init(0, -190)\n",
    "\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write PCA output in csv\n",
    "pc_d=pd.DataFrame(principalComponents)\n",
    "pc_d.to_csv('principalComponents.csv')\n",
    "\n",
    "l=pd.DataFrame({'CDR_score':labels,\n",
    "                    'gender': gender})\n",
    "l.to_csv('data_labels.csv')\n",
    "\n",
    "p=pd.DataFrame(pca.components_)\n",
    "p.to_csv('PCAweights.csv')\n",
    "\n",
    "v=pd.DataFrame(pca.explained_variance_ratio_)\n",
    "v.to_csv('PCAvariance.csv')\n",
    "\n",
    "col = pd.DataFrame(df.columns)\n",
    "col.to_csv('PCAcolumns.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass classification using SVM (all features) 10-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.matlib\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "\n",
    "# 1. prepare data \n",
    "group = np.array(labels)\n",
    "\n",
    "# get indices for different classes\n",
    "for g in np.unique(group):\n",
    "    ix = np.where(group == g)\n",
    "    if g == 0:\n",
    "        class_1 = ix\n",
    "    if g == 0.5: \n",
    "        class_2 = ix\n",
    "    if g == 1:\n",
    "        class_3 = ix\n",
    "    if g == 2: \n",
    "        class_4 = ix\n",
    "    if g == 3:\n",
    "        class_5 = ix\n",
    "        \n",
    "# Get data for each class and add class label in the last column\n",
    "dat_1 = np.concatenate((np.squeeze(principalComponents[class_1,:]),np.ones([len(np.squeeze(class_1)),1])),axis=1)\n",
    "dat_2 = np.concatenate((np.squeeze(principalComponents[class_2,:]),np.ones([len(np.squeeze(class_2)),1])*2),axis=1)\n",
    "dat_3 = np.concatenate((np.squeeze(principalComponents[class_3,:]),np.ones([len(np.squeeze(class_3)),1])*3),axis=1)\n",
    "dat_4 = np.concatenate((np.squeeze(principalComponents[class_4,:]),np.ones([len(np.squeeze(class_4)),1])*4),axis=1)\n",
    "dat_5 = np.concatenate((np.squeeze(principalComponents[class_5,:]),np.ones([len(np.squeeze(class_5)),1])*5),axis=1)\n",
    "\n",
    "# super sampling: repeat matrices to match number of samples in largest class\n",
    "dat_1 = np.matlib.repmat(dat_1,int(np.round(dat_2.shape[0]/dat_1.shape[0])),1)\n",
    "dat_3 = np.matlib.repmat(dat_3,int(np.round(dat_2.shape[0]/dat_3.shape[0])),1)\n",
    "dat_4 = np.matlib.repmat(dat_4,int(np.round(dat_2.shape[0]/dat_4.shape[0])),1)\n",
    "dat_5 = np.matlib.repmat(dat_5,int(np.round(dat_2.shape[0]/dat_5.shape[0])),1)\n",
    "\n",
    "# collect dat from all classes together and shuffle the order\n",
    "all_dat = np.concatenate((dat_1,dat_2,dat_3,dat_4,dat_5),axis=0)\n",
    "random.shuffle(all_dat)\n",
    "\n",
    "# train-test split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(all_dat[:,0:-1], all_dat[:,-1], test_size=0.2)\n",
    "\n",
    "# Get splits for Kfold cross-validation\n",
    "kf = KFold(n_splits = 10, shuffle = True, random_state = 124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Classification with SVM\n",
    "\n",
    "# Define the model\n",
    "clf = svm.SVC(kernel='rbf', C=0.5) # C is regularisation\n",
    "\n",
    "# Cross-validation and training\n",
    "\n",
    "X_skf = all_dat[:,0:-1] # data\n",
    "Y_skf = all_dat[:,-1]   # labels\n",
    "\n",
    "# Normalise data\n",
    "X_skf = MinMaxScaler().fit_transform(X_skf) # normalizing the features\n",
    "\n",
    "fold_no = 0\n",
    "cm =np.zeros([5,5]) # initialise empty confusion matrix\n",
    "classi_report = []\n",
    "train_score = []\n",
    "test_score = []\n",
    "loss = []\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "cnt = 0\n",
    "\n",
    "for train_idx, test_idx in skf.split(X_skf, Y_skf): \n",
    "    \n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} for SVM ...')\n",
    "    # Fit the model\n",
    "    clf.fit(X_skf[train_idx,:], Y_skf[train_idx])\n",
    "\n",
    "    # Make Prediction\n",
    "    y_pred = clf.predict(X_skf[test_idx,:])\n",
    "    \n",
    "    # Making the Confusion Matrix and record performance\n",
    "    cm = cm + confusion_matrix(Y_skf[test_idx],y_pred)\n",
    "    classi_report.append(classification_report(Y_skf[test_idx],y_pred))\n",
    "    \n",
    "    train_score.append(clf.score(X_skf[train_idx,:], Y_skf[train_idx]))\n",
    "    test_score.append(clf.score(X_skf[test_idx,:], Y_skf[test_idx]))\n",
    "    loss.append(metrics.mean_squared_error(Y_skf[test_idx], y_pred))\n",
    "    \n",
    "    all_y_true.append(Y_skf[test_idx])\n",
    "    all_y_pred.append(y_pred)\n",
    "    \n",
    "    print(cm)\n",
    "    print(\"\\n\")\n",
    "    print(classi_report[cnt])\n",
    "\n",
    "    print(\"Training set score for SVM: %f\" %train_score[cnt] )\n",
    "    print(\"Testing  set score for SVM: %f\" %test_score[cnt] )\n",
    "    print(\"Loss per fold: %f\" %loss[cnt])\n",
    "    cnt=cnt+1\n",
    "  \n",
    "    fold_no+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for multiclass classifier (%):\n",
      "\n",
      "                No dementia  Questionable  Mild   Moderate  Severe  \n",
      "No dementia     99.61        0.39          0.0    0.0       0.0\n",
      "Questionable    13.98        85.32         0.7    0.0       0.0\n",
      "Mild            0.0          1.44          98.56  0.0       0.0\n",
      "Moderate        0.0          0.0           0.0    100.0     0.0\n",
      "Severe.         0.0          0.0           0.0    0.0       100.0\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Model Performance: \n",
      "\n",
      "Class                       Precision       Recall      f1-score\n",
      "No dementia                 93.94           99.61       96.69\n",
      "Questionable impairment     97.97           85.32       91.21\n",
      "Mild dementia               98.89           98.56       98.73\n",
      "Moderate dementia           100.0           100.0       100.0\n",
      "Severe dementia             100.0           100.0       100.0\n",
      "\n",
      "Accuracy on test data: 96.10%\n",
      "Accuracy on training data: 98.41%\n",
      "\n",
      "RMSE Loss: 0.039\n"
     ]
    }
   ],
   "source": [
    "# Print results \n",
    "\n",
    "CM = confusion_matrix(np.hstack(all_y_true),np.hstack(all_y_pred),normalize='true')*100\n",
    "CM = np.round(CM,2)\n",
    "print(\"Confusion matrix for multiclass classifier (%):\\n\")\n",
    "print(\"                No dementia  \"+ 'Questionable  '+ 'Mild   '+ 'Moderate  '+ 'Severe  ')\n",
    "print(\"No dementia     \"+ str(CM[0,0]) + \"        \" + str(CM[0,1])+ \"          \" + str(CM[0,2])+ \"    \" + str(CM[0,3])+ \"       \" + str(CM[0,4]))\n",
    "print(\"Questionable    \"+ str(CM[1,0]) + \"        \" + str(CM[1,1])+ \"         \" + str(CM[1,2])+ \"    \" + str(CM[1,3])+ \"       \" + str(CM[1,4]))\n",
    "print(\"Mild            \"+ str(CM[2,0]) + \"          \" + str(CM[2,1])+ \"          \" + str(CM[2,2])+ \"  \" + str(CM[2,3])+ \"       \" + str(CM[2,4]))\n",
    "print(\"Moderate        \"+ str(CM[3,0]) + \"          \" + str(CM[3,1])+ \"           \" + str(CM[3,2])+ \"    \" + str(CM[3,3])+ \"     \" + str(CM[3,4]))\n",
    "print(\"Severe.         \"+ str(CM[4,0]) + \"          \" + str(CM[4,1])+ \"           \" + str(CM[4,2])+ \"    \" + str(CM[4,3])+ \"       \" + str(CM[4,4]))\n",
    "\n",
    "print('--------------------------------------------------------------------')\n",
    "print()\n",
    "print('Model Performance: ')\n",
    "print()\n",
    "report = classification_report(np.hstack(all_y_true),np.hstack(all_y_pred), output_dict=True)\n",
    "print(\"Class                       Precision       Recall      f1-score\")\n",
    "print(\"No dementia                 \"+ str(np.round(report['1.0']['precision']*100,2))+\"           \"+\n",
    "                                      str(np.round(report['1.0']['recall']*100,2)) +\"       \"+\n",
    "                                      str(np.round(report['1.0']['f1-score']*100,2)))\n",
    "print(\"Questionable impairment     \"+ str(np.round(report['2.0']['precision']*100,2))+\"           \"+\n",
    "                                      str(np.round(report['2.0']['recall']*100,2)) +\"       \"+\n",
    "                                      str(np.round(report['2.0']['f1-score']*100,2)))\n",
    "print(\"Mild dementia               \"+ str(np.round(report['3.0']['precision']*100,2))+\"           \"+\n",
    "                                      str(np.round(report['3.0']['recall']*100,2)) +\"       \"+\n",
    "                                      str(np.round(report['3.0']['f1-score']*100,2)))\n",
    "print(\"Moderate dementia           \"+ str(np.round(report['4.0']['precision']*100,2))+\"           \"+\n",
    "                                      str(np.round(report['4.0']['recall']*100,2)) +\"       \"+\n",
    "                                      str(np.round(report['4.0']['f1-score']*100,2)))\n",
    "print(\"Severe dementia             \"+ str(np.round(report['5.0']['precision']*100,2))+\"           \"+\n",
    "                                      str(np.round(report['5.0']['recall']*100,2)) +\"       \"+\n",
    "                                      str(np.round(report['5.0']['f1-score']*100,2)))\n",
    "print()\n",
    "print(\"Accuracy on test data: %0.2f\" %np.round(np.mean(test_score)*100,2)+'%')\n",
    "print(\"Accuracy on training data: %0.2f\" %np.round(np.mean(train_score)*100,2)+'%')\n",
    "print()\n",
    "print(\"RMSE Loss: %0.3f\" %np.mean(loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
